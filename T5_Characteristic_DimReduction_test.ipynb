{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "201bd9f6",
   "metadata": {},
   "source": [
    "# Make sure the existance of all poster images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3160360f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Check how many posters are missing\n",
    "downloaded = set(int(f.split('.')[0]) for f in os.listdir('posters_test') if f.endswith('.jpg'))\n",
    "all_movies = set(pd.read_csv('movies_test.csv')['movieId'])\n",
    "missing_count = len(all_movies - downloaded)\n",
    "\n",
    "print(f\"Missing posters: {missing_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121d121b",
   "metadata": {},
   "source": [
    "# Feature extraction and Dim reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd9ef6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "HOG Feature Extraction + LDA Reduction for Test Posters\n",
      "======================================================================\n",
      "\n",
      "1. Extracting HOG features from test posters...\n",
      "Found 1497 poster files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting HOG features: 100%|██████████| 1497/1497 [01:07<00:00, 22.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved raw HOG features: (1497, 8100) -> posters_test\\feature_HOG.csv\n",
      "\n",
      "============================================================\n",
      "HOG Feature Extraction Completed!\n",
      "Total processed: 1497\n",
      "Failed images: 0\n",
      "============================================================\n",
      "\n",
      "2. Applying LDA dimensionality reduction...\n",
      "\n",
      "Applying LDA reduction to 18 dimensions...\n",
      "✓ LDA trained with 19 genre classes\n",
      "✓ LDA reduction applied: 8100 -> 18 dimensions\n",
      "\n",
      "3. Saving final reduced features...\n",
      "✓ Saved reduced features: HOG_lda_18d.csv (18 components)\n",
      "\n",
      "======================================================================\n",
      "✅ PROCESSING COMPLETED!\n",
      "======================================================================\n",
      "Input posters: posters_test/\n",
      "Raw HOG features: posters_test/feature_HOG.csv\n",
      "Final features: features_test/HOG_lda_18d.csv\n",
      "HOG feature dimension: 1764\n",
      "LDA reduced dimension: 18\n",
      "Total movies processed: 1497\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Feature Extraction and Dimensionality Reduction for Test Posters\n",
    "Extracts HOG features and applies LDA reduction to 18 dimensions\n",
    "\"\"\"\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from skimage.feature import hog\n",
    "from skimage.color import rgb2gray\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ========================================\n",
    "# FEATURE EXTRACTION - HOG\n",
    "# ========================================\n",
    "\n",
    "class HOGFeatureExtractor:\n",
    "    \"\"\"Extract HOG features from movie posters\"\"\"\n",
    "    \n",
    "    def __init__(self, posters_dir='posters_test', features_dir='posters_test'):\n",
    "        self.posters_dir = Path(posters_dir)\n",
    "        self.features_dir = Path(features_dir)\n",
    "        self.features_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "    def extract_hog_features(self, image, orientations=9, pixels_per_cell=(16, 16), cells_per_block=(2, 2)):\n",
    "        \"\"\"\n",
    "        Extract HOG (Histogram of Oriented Gradients) features\n",
    "        Args:\n",
    "            image: BGR image from cv2\n",
    "            orientations: number of orientation bins\n",
    "            pixels_per_cell: size of a cell\n",
    "            cells_per_block: number of cells in each block\n",
    "        Returns:\n",
    "            HOG feature vector (1764 features)\n",
    "        \"\"\"\n",
    "        # Convert to grayscale\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Resize to standard size for consistency\n",
    "        gray = cv2.resize(gray, (256, 256))\n",
    "        \n",
    "        # Extract HOG features\n",
    "        features = hog(gray, \n",
    "                      orientations=orientations, \n",
    "                      pixels_per_cell=pixels_per_cell,\n",
    "                      cells_per_block=cells_per_block,\n",
    "                      visualize=False, \n",
    "                      feature_vector=True)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def load_image(self, image_path):\n",
    "        \"\"\"Load and validate image\"\"\"\n",
    "        try:\n",
    "            image = cv2.imread(str(image_path))\n",
    "            if image is None:\n",
    "                return None\n",
    "            return image\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {image_path}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def extract_hog_features_all(self):\n",
    "        \"\"\"\n",
    "        Extract HOG features for all test posters\n",
    "        Returns:\n",
    "            DataFrame with movieId and HOG features\n",
    "        \"\"\"\n",
    "        # Get all poster files\n",
    "        poster_files = sorted(list(self.posters_dir.glob('*.jpg')))\n",
    "        \n",
    "        if not poster_files:\n",
    "            print(f\"No poster files found in {self.posters_dir}\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"Found {len(poster_files)} poster files\")\n",
    "        \n",
    "        hog_features = []\n",
    "        movieIds = []\n",
    "        failed_images = []\n",
    "        \n",
    "        # Process each poster\n",
    "        for poster_file in tqdm(poster_files, desc=\"Extracting HOG features\"):\n",
    "            # Extract movieId from filename (e.g., \"123.jpg\" -> 123)\n",
    "            try:\n",
    "                movie_id = int(poster_file.stem)\n",
    "            except ValueError:\n",
    "                print(f\"Skipping invalid filename: {poster_file.name}\")\n",
    "                continue\n",
    "            \n",
    "            # Load image\n",
    "            image = self.load_image(poster_file)\n",
    "            if image is None:\n",
    "                failed_images.append(movie_id)\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # Extract HOG features\n",
    "                hog_feat = self.extract_hog_features(image)\n",
    "                \n",
    "                # Store features\n",
    "                movieIds.append(movie_id)\n",
    "                hog_features.append(hog_feat)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {poster_file.name}: {e}\")\n",
    "                failed_images.append(movie_id)\n",
    "                continue\n",
    "        \n",
    "        # Create DataFrame\n",
    "        if hog_features:\n",
    "            features_array = np.array(hog_features)\n",
    "            feature_cols = [f'HOG_feat_{i}' for i in range(features_array.shape[1])]\n",
    "            df = pd.DataFrame(features_array, columns=feature_cols)\n",
    "            df.insert(0, 'movieId', movieIds)\n",
    "            \n",
    "            # Save raw HOG features\n",
    "            output_path = self.features_dir / 'feature_HOG.csv'\n",
    "            df.to_csv(output_path, index=False)\n",
    "            print(f\"✓ Saved raw HOG features: {features_array.shape} -> {output_path}\")\n",
    "            \n",
    "            # Report results\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"HOG Feature Extraction Completed!\")\n",
    "            print(f\"Total processed: {len(movieIds)}\")\n",
    "            print(f\"Failed images: {len(failed_images)}\")\n",
    "            if failed_images:\n",
    "                print(f\"Failed movieIds: {failed_images[:10]}{'...' if len(failed_images) > 10 else ''}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            return df\n",
    "        else:\n",
    "            print(\"✗ No features extracted!\")\n",
    "            return None\n",
    "\n",
    "# ========================================\n",
    "# DIMENSIONALITY REDUCTION - LDA\n",
    "# ========================================\n",
    "\n",
    "class LDA_FromScratch:\n",
    "    \"\"\"LDA Implementation from Scratch\"\"\"\n",
    "    \n",
    "    def __init__(self, n_components=None):\n",
    "        self.n_components = n_components\n",
    "        self.components_ = None\n",
    "        self.mean_ = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit LDA on data X with labels y\"\"\"\n",
    "        n_features = X.shape[1]\n",
    "        class_labels = np.unique(y)\n",
    "        n_classes = len(class_labels)\n",
    "        \n",
    "        if self.n_components is None:\n",
    "            self.n_components = min(n_features, n_classes - 1)\n",
    "        else:\n",
    "            self.n_components = min(self.n_components, n_classes - 1)\n",
    "        \n",
    "        self.mean_ = np.mean(X, axis=0)\n",
    "        S_W = np.zeros((n_features, n_features))\n",
    "        S_B = np.zeros((n_features, n_features))\n",
    "        \n",
    "        for c in class_labels:\n",
    "            X_c = X[y == c]\n",
    "            mean_c = np.mean(X_c, axis=0)\n",
    "            n_c = X_c.shape[0]\n",
    "            S_W += np.dot((X_c - mean_c).T, (X_c - mean_c))\n",
    "            mean_diff = (mean_c - self.mean_).reshape(-1, 1)\n",
    "            S_B += n_c * np.dot(mean_diff, mean_diff.T)\n",
    "        \n",
    "        try:\n",
    "            S_W_reg = S_W + np.eye(n_features) * 1e-6\n",
    "            eigenvalues, eigenvectors = np.linalg.eig(np.dot(np.linalg.inv(S_W_reg), S_B))\n",
    "            idx = np.argsort(np.abs(eigenvalues))[::-1]\n",
    "            eigenvalues = eigenvalues[idx]\n",
    "            eigenvectors = eigenvectors[:, idx]\n",
    "            self.components_ = np.real(eigenvectors[:, :self.n_components]).T\n",
    "        except np.linalg.LinAlgError:\n",
    "            print(\"  ⚠ Warning: LDA encountered numerical issues, using fallback\")\n",
    "            # Fallback to PCA-like approach\n",
    "            cov_matrix = np.dot(X.T, X) / (X.shape[0] - 1)\n",
    "            eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n",
    "            idx = np.argsort(eigenvalues)[::-1]\n",
    "            eigenvectors = eigenvectors[:, idx]\n",
    "            self.components_ = eigenvectors[:, :self.n_components].T\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Transform data using fitted LDA\"\"\"\n",
    "        return np.dot(X, self.components_.T)\n",
    "    \n",
    "    def fit_transform(self, X, y):\n",
    "        \"\"\"Fit and transform in one step\"\"\"\n",
    "        self.fit(X, y)\n",
    "        return self.transform(X)\n",
    "\n",
    "def load_train_genres(train_csv_path='movies_train.csv'):\n",
    "    \"\"\"Load genre information from training data for LDA\"\"\"\n",
    "    df = pd.read_csv(train_csv_path)\n",
    "    genre_dict = {}\n",
    "    for _, row in df.iterrows():\n",
    "        movie_id = row['movieId']\n",
    "        genres = row['genres']\n",
    "        if pd.isna(genres) or genres == '(no genres listed)':\n",
    "            primary_genre = 'Unknown'\n",
    "        else:\n",
    "            primary_genre = genres.split('|')[0]\n",
    "        genre_dict[movie_id] = primary_genre\n",
    "    return genre_dict\n",
    "\n",
    "def normalize_features(features):\n",
    "    \"\"\"Normalize features using StandardScaler\"\"\"\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    normalized = scaler.fit_transform(features)\n",
    "    return normalized, scaler\n",
    "\n",
    "def apply_lda_reduction(hog_features_df, train_csv='movies_train.csv', n_components=18):\n",
    "    \"\"\"\n",
    "    Apply LDA dimensionality reduction to HOG features\n",
    "    \"\"\"\n",
    "    print(f\"\\nApplying LDA reduction to {n_components} dimensions...\")\n",
    "    \n",
    "    # Load training data for LDA fitting\n",
    "    train_features_path = 'features/feature_HOG.csv'  # Usar datos de entrenamiento para entrenar LDA\n",
    "    if not Path(train_features_path).exists():\n",
    "        print(f\"✗ Error: Training features not found at {train_features_path}\")\n",
    "        print(\"  LDA needs training data to learn the transformation\")\n",
    "        return None\n",
    "    \n",
    "    # Cargar características de entrenamiento\n",
    "    train_hog_df = pd.read_csv(train_features_path)\n",
    "    train_movie_ids = train_hog_df['movieId'].values\n",
    "    train_features = train_hog_df.drop('movieId', axis=1).values\n",
    "    \n",
    "    # Cargar géneros de entrenamiento\n",
    "    genre_dict = load_train_genres(train_csv)\n",
    "    train_labels = np.array([genre_dict.get(mid, 'Unknown') for mid in train_movie_ids])\n",
    "    \n",
    "    # Normalizar características de entrenamiento\n",
    "    train_features_norm, scaler = normalize_features(train_features)\n",
    "    \n",
    "    # Entrenar LDA con datos de entrenamiento\n",
    "    lda = LDA_FromScratch(n_components=n_components)\n",
    "    lda.fit(train_features_norm, train_labels)\n",
    "    print(f\"✓ LDA trained with {len(np.unique(train_labels))} genre classes\")\n",
    "    \n",
    "    # Preparar características de test\n",
    "    test_movie_ids = hog_features_df['movieId'].values\n",
    "    test_features = hog_features_df.drop('movieId', axis=1).values\n",
    "    \n",
    "    # Normalizar características de test con el mismo scaler\n",
    "    test_features_norm = scaler.transform(test_features)\n",
    "    \n",
    "    # Aplicar transformación LDA\n",
    "    reduced_features = lda.transform(test_features_norm)\n",
    "    \n",
    "    print(f\"✓ LDA reduction applied: {test_features.shape[1]} -> {reduced_features.shape[1]} dimensions\")\n",
    "    \n",
    "    return test_movie_ids, reduced_features\n",
    "\n",
    "def save_reduced_features(movie_ids, reduced_features, output_path):\n",
    "    \"\"\"Save reduced features to CSV\"\"\"\n",
    "    n_components = reduced_features.shape[1]\n",
    "    df = pd.DataFrame(reduced_features, columns=[f'comp_{i}' for i in range(n_components)])\n",
    "    df.insert(0, 'movieId', movie_ids)\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"✓ Saved reduced features: {output_path.name} ({reduced_features.shape[1]} components)\")\n",
    "    return df\n",
    "\n",
    "# ========================================\n",
    "# MAIN PIPELINE\n",
    "# ========================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"HOG Feature Extraction + LDA Reduction for Test Posters\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Initialize extractor\n",
    "    extractor = HOGFeatureExtractor(posters_dir='posters_test', features_dir='posters_test')\n",
    "    \n",
    "    # Step 1: Extract HOG features\n",
    "    print(\"\\n1. Extracting HOG features from test posters...\")\n",
    "    hog_df = extractor.extract_hog_features_all()\n",
    "    \n",
    "    if hog_df is None:\n",
    "        print(\"✗ Failed to extract HOG features\")\n",
    "        return\n",
    "    \n",
    "    # Step 2: Apply LDA reduction\n",
    "    print(\"\\n2. Applying LDA dimensionality reduction...\")\n",
    "    result = apply_lda_reduction(hog_df, n_components=18)\n",
    "    \n",
    "    if result is None:\n",
    "        print(\"✗ Failed to apply LDA reduction\")\n",
    "        return\n",
    "    \n",
    "    movie_ids, reduced_features = result\n",
    "    \n",
    "    # Step 3: Save final features\n",
    "    print(\"\\n3. Saving final reduced features...\")\n",
    "    output_path = Path('features_test') / 'HOG_lda_18d.csv'\n",
    "    final_df = save_reduced_features(movie_ids, reduced_features, output_path)\n",
    "    \n",
    "    # Final summary\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"✅ PROCESSING COMPLETED!\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Input posters: posters_test/\")\n",
    "    print(f\"Raw HOG features: posters_test/feature_HOG.csv\")\n",
    "    print(f\"Final features: features_test/HOG_lda_18d.csv\")\n",
    "    print(f\"HOG feature dimension: 1764\")\n",
    "    print(f\"LDA reduced dimension: 18\")\n",
    "    print(f\"Total movies processed: {len(movie_ids)}\")\n",
    "    print(f\"{'='*70}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
